{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter_saliency_bias_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "mldash_entity": {
      "created_at_millis": 1620095915717,
      "hash": "2fcd96a2e80eeefd7a908fa89ef475b56bffe759",
      "inferred_pdp_safe": false,
      "is_vfs_dir": false,
      "marked_pdp_safe": false,
      "owner": "smishra",
      "shared_to_everyone": false,
      "shared_to_ldap_groups": [],
      "shared_to_ldap_users": [],
      "size": 6942268,
      "tags": [],
      "uuid": "1389409090200166402",
      "vfs_path": "/user/smishra/notebooks/ImageCrop/Image Crop Analysis.ipynb"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrYXKydO1vBE"
      },
      "source": [
        "# **Automated algorithmic bias analysis of Twitter saliency filter**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e92VQTMmGC5F"
      },
      "source": [
        "## Author: \n",
        "## [**Dr. Rahul Remanan**](https://linkedin.com/in/rahulremanan), \n",
        "### [**CEO, Moad Computer (A division of Ekaveda Inc.)**](https://moad.computer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJspz9LIF6gM"
      },
      "source": [
        "This notebook introduces a few broad concepts, that will help develop automated testing tools to detect algorithmic bias in machine vision tools, such as saliency filters.\n",
        "\n",
        "The tool evaluated here is the [Twitter saliency filter](https://github.com/twitter-research/image-crop-analysis).\n",
        "\n",
        "[FairFace: the face attribute dataset that is balanced for gender, race and age](https://arxiv.org/abs/1908.04913v1); is used here to generate the random image pairs for performing the saliency filter tests.\n",
        "\n",
        "Quantification of the statisitcal significance in differences between the carefully manipulated saliency filter outputs and the baseline saliency filter outputs, is performed using the [Wilcoxon signed rank test](https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB5g2-JO8DMz"
      },
      "source": [
        "### Additional requirements\n",
        "\n",
        "* Valid Google account\n",
        "* This notebook by default assumes that the user is working inside the original [Google Colab environment](https://colab.research.google.com/drive/1eZpt6KPtrlA2egvuTnyS31v3UqDJScCD?usp=sharing). To run locally or in other cloud environments, please make sure that the data dependencies are satisfied.\n",
        "* Google Drive access to save the FairFace dataset and the experiment history\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkG0Jk8EJ2tM"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1eZpt6KPtrlA2egvuTnyS31v3UqDJScCD?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zo2K1zloOEN"
      },
      "source": [
        "```\n",
        "Parts of the code used in this notebook are copyright protected.\n",
        "Copyright 2021 Twitter, Inc.\n",
        "SPDX-License-Identifier: Apache-2.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i7DzgcsG0BF"
      },
      "source": [
        "# Install Twitter saliency filter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1JP_2nhJ2tc"
      },
      "source": [
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "BIN_MAPS = {\"Darwin\": \"mac\", \"Linux\": \"linux\"}\n",
        "\n",
        "HOME_DIR = Path(\"../\").expanduser()\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  !python3 -m pip install -q pandas scikit-learn scikit-image statsmodels requests dash\n",
        "  ![[ -d image-crop-analysis ]] || git clone https://github.com/twitter-research/image-crop-analysis.git\n",
        "  HOME_DIR = Path(\"./image-crop-analysis\").expanduser()\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTusTzvqhUz8"
      },
      "source": [
        "import sys, platform\n",
        "sys.path.append(str(HOME_DIR / \"src\"))\n",
        "bin_dir = HOME_DIR / Path(\"./bin\")\n",
        "bin_path = bin_dir / BIN_MAPS[platform.system()] / \"candidate_crops\"\n",
        "model_path = bin_dir / \"fastgaze.vxm\"\n",
        "data_dir = HOME_DIR / Path(\"./data/\")\n",
        "data_dir.exists()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDQ1oY7TGwGi"
      },
      "source": [
        "# Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR4WEPe18t-R"
      },
      "source": [
        "import os,gc,json,glob,shlex,random,platform,warnings,subprocess,numpy as np, \\\n",
        "       pandas as pd,matplotlib.pyplot as plt,matplotlib.image as mpimg\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "from scipy.stats import wilcoxon\n",
        "from collections import namedtuple\n",
        "from matplotlib.patches import Rectangle\n",
        "from image_manipulation import join_images\n",
        "from matplotlib.collections import PatchCollection\n",
        "from crop_api import ImageSaliencyModel, is_symmetric, parse_output, reservoir_sampling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GBUrqiRIiS2"
      },
      "source": [
        "# Mount Google Drive\n",
        "\n",
        "By default this notebook assumes that the FairFace dataset is stored in the Google Drive attached here. Also, the experimental histories are saved to the Google Drive attached to this Colab notebook in `csv` format.\n",
        "\n",
        "## Data download\n",
        "Download the FairFace dataset **`fairface-img-margin125-trainval.zip`** file and the labels **`fairface_label_train.csv`** file from the official **[FairFace GitHub repo](https://github.com/joojs/fairface)**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3r1a414gcSv"
      },
      "source": [
        "img_dir = './'\n",
        "if IN_COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  img_dir = '/content/drive/MyDrive/'\n",
        "fairface_dir = f'{img_dir}/FairFace/'\n",
        "if not os.path.exists(f'{fairface_dir}/fairface-img-margin125-trainval.zip'):\n",
        "  raise ValueError(f'Please check whether the FairFace dataset zip file exists at: {fairface_dir}/fairface-img-margin125-trainval.zip')\n",
        "if not os.path.exists(f'{fairface_dir}/fairface_label_train.csv'):\n",
        "  raise ValueError(f'Please check whether the FairFace data labels csv file exists at: {fairface_dir}/fairface_label_train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa1Ju4-IIxRs"
      },
      "source": [
        "# FairFace helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqOwqJXOjLq7"
      },
      "source": [
        "def random_imgID_generator(df, pairs=True):\n",
        "  num_images = len(df)\n",
        "  id1 = random.SystemRandom().choice(range(0,num_images))\n",
        "  if pairs:\n",
        "    id2 = random.SystemRandom().choice(range(0,num_images))\n",
        "    return id1, id2\n",
        "  return id1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYHRE-ADrots"
      },
      "source": [
        "def eval_conditions(df, id1, id2):\n",
        "  id_condition = id1 == id2\n",
        "  race_condition = str(df.iloc[id2].race).lower()==str(df.iloc[id1].race).lower()\n",
        "  return id_condition, race_condition"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isk3jQqgRa-U"
      },
      "source": [
        "def img_pairs_filter(df,id1,id2,max_retries=100):\n",
        "  id_condition, race_condition = eval_conditions(df, id1, id2)\n",
        "  if id_condition or race_condition:\n",
        "    for i in tqdm(range(max_retries)):\n",
        "      id2 = random_imgID_generator(df, pairs=False)\n",
        "      tqdm.write(f'FairFace pair generation attempt {i+1}/{max_retries}')\n",
        "      id_condition, race_condition = eval_conditions(df, id1, id2)\n",
        "      if not id_condition and not race_condition:\n",
        "        break\n",
        "    print(f'Generated FairFace pairs in attempt: {i+1}/{max_retries}')    \n",
        "  print(f'FairFace images {id1+1} and {id2+1} selected for evaluation using Twitter Saliency algorithm ...\\n')\n",
        "  return id1, id2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFAFoMd2ho0y"
      },
      "source": [
        "def img_info(df, id1, id2=None, verbose=False):\n",
        "  if verbose:\n",
        "    print(f'Labels for {id1+1} ...\\n')\n",
        "    print(df.iloc[id1])\n",
        "    print('\\n','-'*32)\n",
        "  info1 = { 'file': df['file'].iloc[id1].split('/')[-1].replace('.jpg',''),\n",
        "            'race': df['race'].iloc[id1],\n",
        "            'gender': df['gender'].iloc[id1],\n",
        "            'age': df['age'].iloc[id1] }\n",
        "  if id2 is not None:\n",
        "    info2 = { 'file': df['file'].iloc[id2].split('/')[-1].replace('.jpg',''),\n",
        "              'race': df['race'].iloc[id2],\n",
        "              'gender': df['gender'].iloc[id2],\n",
        "              'age': df['age'].iloc[id2] }\n",
        "    if verbose:\n",
        "      print(f'\\nLabels for {id2+1} ...\\n')\n",
        "      print(df.iloc[id2])\n",
        "    return info1, info2\n",
        "  return info1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e5j2kYOcHC_"
      },
      "source": [
        "def execute_in_shell(command, verbose=False):\n",
        "    \"\"\" \n",
        "        command -- keyword argument, takes a list as input\n",
        "        verbsoe -- keyword argument, takes a boolean value as input\n",
        "    \n",
        "        This is a function that executes shell scripts from within python.\n",
        "        \n",
        "        Keyword argument 'command', should be a list of shell commands.\n",
        "        Keyword argument 'versboe', should be a boolean value to set verbose level.\n",
        "        \n",
        "        Example usage: execute_in_shell(command = ['ls ./some/folder/',\n",
        "                                                    ls ./some/folder/  -1 | wc -l'],\n",
        "                                        verbose = True ) \n",
        "                                        \n",
        "        This command returns dictionary with elements: Output and Error.\n",
        "        \n",
        "        Output records the console output,\n",
        "        Error records the console error messages.\n",
        "                                        \n",
        "    \"\"\"\n",
        "    error = []\n",
        "    output = []\n",
        "    \n",
        "    if isinstance(command, list):\n",
        "        for i in range(len(command)):\n",
        "            try:\n",
        "                process = subprocess.Popen(command[i], shell=True, stdout=subprocess.PIPE)\n",
        "                process.wait()\n",
        "                out, err = process.communicate()\n",
        "                error.append(err)\n",
        "                output.append(out)\n",
        "                if verbose:\n",
        "                    print ('Success running shell command: {}'.format(command[i]))\n",
        "            except Exception as e:\n",
        "                print ('Failed running shell command: {}'.format(command[i]))\n",
        "                if verbose:\n",
        "                    print(type(e))\n",
        "                    print(e.args)\n",
        "                    print(e)\n",
        "                    print(logging.error(e, exc_info=True))\n",
        "    else:\n",
        "        raise ValueError('Expects a list input ...')\n",
        "    return {'Output': output, 'Error': error }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMNHODLogfg8"
      },
      "source": [
        "def clear_image_history(out_dir):\n",
        "   _ = execute_in_shell([f'rm -r {out_dir}/*.jpg'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0NY-Ighckum"
      },
      "source": [
        "def get_fairface_img(df, img_id, out_dir, fairface_data):\n",
        "  file_ = str(df.iloc[img_id].file)\n",
        "  _ = execute_in_shell([f'unzip -j -q {fairface_data} {file_} -d {out_dir}'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAyJ9ma8Sc8d"
      },
      "source": [
        "def randomID_generator():\n",
        "  return ''.join(\n",
        "           random.SystemRandom().sample(\n",
        "             list(\n",
        "               'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmopqrstuvwxyz0123456789'\n",
        "               ),8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD6A8ljj6Kjl"
      },
      "source": [
        "def fairface_data_checks(fairface_data):\n",
        "  if not os.path.exists(fairface_data):\n",
        "    raise ValueError(f\"Couldn't find FairFace data archive: {fairface_data}. \\nPlease download FairFace data from: https://github.com/joojs/fairface and save the zip file in: {fairface_dir}\")\n",
        "  fairface_labels = f'{fairface_dir}/fairface_label_train.csv'\n",
        "  if not os.path.exists(fairface_labels):\n",
        "    raise ValueError(f\"Couldn't find FairFace data labels: {fairface_labels}. \\nPlease download FairFace data labels from: https://github.com/joojs/fairface and save the csv file in: {fairface_labels}\")\n",
        "  return fairface_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HS8oBLaI1WK"
      },
      "source": [
        "# Read FairFace data\n",
        "The FairFace dataset should be downloaded and placed insides the `{img_dir}/FairFace` directory. By default the notebook uses the `fairface-img-margin125-trainval.zip` FairFace data zip archive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5pry8aDlk82"
      },
      "source": [
        "unzip_dir = str(data_dir.absolute())\n",
        "fairface_data = f'{fairface_dir}/fairface-img-margin125-trainval.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_Jbc-V36vpq"
      },
      "source": [
        "## Checks for FairFace data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_PkBHrSINNf"
      },
      "source": [
        "img_labels = pd.read_csv(fairface_data_checks(fairface_data))\n",
        "img_labels.head()\n",
        "num_images = len(img_labels)\n",
        "print(f'Total number of FairFace images: {num_images}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lh4FvKzIGuz"
      },
      "source": [
        "# Generate random face pairings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPzSX48gVirQ"
      },
      "source": [
        "img_idx1,img_idx2 = random_imgID_generator(img_labels)  \n",
        "max_retries = 2000\n",
        "img_idx1, img_idx2 = img_pairs_filter(img_labels,img_idx1,img_idx2,\n",
        "                                      max_retries=max_retries)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXZyEF1q7hEF"
      },
      "source": [
        "img_info(img_labels, img_idx1, img_idx2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVpvoWdpUn0E"
      },
      "source": [
        "# Numerical encoding of the FairFace labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTDdqYQB33rm"
      },
      "source": [
        "twitter_saliency_eval_dir = f'{img_dir}//Twitter_saliency'\n",
        "if not os.path.exists(twitter_saliency_eval_dir):\n",
        "  print(f'No outputs directory: {twitter_saliency_eval_dir} found ...')\n",
        "  execute_in_shell([f'mkdir {twitter_saliency_eval_dir}'])\n",
        "  print(f'Created outputs directory: {twitter_saliency_eval_dir}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqmmIHKUGbJ7"
      },
      "source": [
        "labels_encoder_file = f'{twitter_saliency_eval_dir}/labels_encoder.json'\n",
        "if os.path.exists(labels_encoder_file):\n",
        "  with open(labels_encoder_file) as f:\n",
        "    labels_encoder = json.loads(f.read())\n",
        "  print(labels_encoder)\n",
        "  print(f'Loaded labels encoder data from: {labels_encoder_file} ...')  \n",
        "else:\n",
        "  print(f'No saved labels encoder data: {labels_encoder_file} ...')\n",
        "  labels_encoder = {}\n",
        "  for i, race in enumerate(sorted(list(set(img_labels['race'].values)))):\n",
        "    labels_encoder.update({race: i})\n",
        "  print(labels_encoder)\n",
        "  with open(labels_encoder_file, 'w+') as f:\n",
        "    json.dump(labels_encoder, f)\n",
        "  print(f'Saved labels encoder data to: {labels_encoder_file} ...')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNoPkJLHJhcC"
      },
      "source": [
        "def encoded_labels(input_label, labels_encoder):\n",
        "  return labels_encoder[input_label]\n",
        "def decoded_labels(input_label, labels_encoder):\n",
        "  return list(labels_encoder.keys())[list(labels_encoder.values()).index(input_label)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RygPE41sYgnV"
      },
      "source": [
        "# Build pairwise image comparisons using the Twitter saliency filter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdVbSHS2fH08"
      },
      "source": [
        "clear_image_history(unzip_dir)\n",
        "get_fairface_img(img_labels, img_idx1, unzip_dir, fairface_data)\n",
        "get_fairface_img(img_labels, img_idx2, unzip_dir, fairface_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se5TLoDVJ2tf"
      },
      "source": [
        "img_path = next(data_dir.glob(\"./*.jpg\"))\n",
        "img_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQgAQqY-J2tg"
      },
      "source": [
        "for img_file in data_dir.glob(\"./*.jpg\"):\n",
        "  img = mpimg.imread(img_file)\n",
        "  plt.figure()\n",
        "  plt.imshow(img)\n",
        "  plt.gca().add_patch(\n",
        "      Rectangle((0, 0), 200, 112, linewidth=1, edgecolor=\"r\", facecolor=\"none\")\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kVVnTvBJ2tp"
      },
      "source": [
        "cmd = f\"{str(bin_path)} {str(model_path)} '{img_path.absolute()}' show_all_points\"\n",
        "cmd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bpi4a4E4J2tr"
      },
      "source": [
        "output = subprocess.check_output(cmd, shell=True)  # Success!\n",
        "print(output.splitlines())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0_nDyaeJ2tt"
      },
      "source": [
        "!{str(bin_path)} {str(model_path)} '{img_path.absolute()}' show_all_points | head"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXmnxzVyJ2tw"
      },
      "source": [
        "parse_output(output).keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBzYy0ScJ2ty"
      },
      "source": [
        "model = ImageSaliencyModel(crop_binary_path=bin_path, crop_model_path=model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG0GGuMnJ2tz"
      },
      "source": [
        "plt.matplotlib.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4zyZRNOJ2t0"
      },
      "source": [
        "list(data_dir.glob(\"./*.jpg\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGAxF1rvJ2t1"
      },
      "source": [
        "for img_path in data_dir.glob(\"*.jpg\"):\n",
        "    print(img_path)\n",
        "    model.plot_img_crops(img_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lii3XE2BJ2t4"
      },
      "source": [
        "for img_path in reservoir_sampling(data_dir.glob(\"./*.jpg\"), K=5):\n",
        "  model.plot_img_crops(img_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVEquaU3J2t_"
      },
      "source": [
        "## Crop an image generated using combination of images\n",
        "\n",
        "* The top 3 crops are sampled based on saliency scores converted into probs using the following formula:\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "p_i = \\frac{exp(s_i)}{Z}\\\\\n",
        "Z = \\sum_{j=0}^{j=N} exp(s_j)\n",
        "\\end{equation}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6w75hK0rsjxY"
      },
      "source": [
        "img_id1 = str(img_labels.iloc[img_idx1].file).split('/')[-1].replace('.jpg','')\n",
        "img_race1 = str(img_labels.iloc[img_idx1].race)\n",
        "img_gender1 = str(img_labels.iloc[img_idx1].gender)\n",
        "img_id2 = str(img_labels.iloc[img_idx2].file).split('/')[-1].replace('.jpg','')\n",
        "img_race2 = str(img_labels.iloc[img_idx2].race)\n",
        "img_gender2 = str(img_labels.iloc[img_idx2].gender)\n",
        "file_id = f'{img_id1}_{img_race1}_{img_gender1}--{img_id2}_{img_race2}_{img_gender2}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R7VzWq4R6Gd"
      },
      "source": [
        "output_dir = './'\n",
        "padding = 0\n",
        "instance_id = randomID_generator()\n",
        "filename = f'{instance_id}_{file_id}_p{padding}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaAB82gfZwHq"
      },
      "source": [
        "# Helper functions to map the saliency filter output to FairFace data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BkB-Hhgj5yq"
      },
      "source": [
        "def saliency_to_image(img, saliency_point, img_files, padding=0, image_mode='horizontal'):\n",
        "  if image_mode == 'horizontal':\n",
        "    saliency_idx = 0\n",
        "  elif image_mode == 'vertical':\n",
        "    saliency_idx = 1\n",
        "  else:\n",
        "    raise ValueError('Unsupported image mode. \\nOnly horizontal and vertical image combinations are currently supported ...')\n",
        "  for i in range(len(img_files)):\n",
        "    if len(saliency_point)>1:\n",
        "      warnings.warn('Only reading the first saliency point. \\nParsing  one saliency point is currently supported ...')\n",
        "    saliency_image_idx =  0  \n",
        "    if (img.size[saliency_idx]-saliency_point[0][saliency_idx]) < (\n",
        "        img.size[saliency_idx]-(i*img.size[saliency_idx]/len(img_files))):\n",
        "       saliency_image_idx = i\n",
        "  if saliency_image_idx < len(img_files):\n",
        "    return img_files[saliency_image_idx]\n",
        "  else:\n",
        "    return img_files[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMS3zxpFpgy0"
      },
      "source": [
        "def saliency_point_to_info(input_file, img_files, model, image_mode='horizontal'):\n",
        "  saliency_point = model.get_output(Path(input_file))['salient_point']\n",
        "  img = Image.open(input_file)\n",
        "  saliency_img_file = saliency_to_image(img, saliency_point, img_files, image_mode=image_mode)\n",
        "  try:\n",
        "    saliency_filename = saliency_img_file.absolute()\n",
        "  except AttributeError:\n",
        "    saliency_filename = str(saliency_img_file)\n",
        "  saliencyID = str(saliency_filename).split('/')[-1].replace('.jpg','')\n",
        "  saliency_info = img_info(img_labels, int(saliencyID)-1)\n",
        "  return saliency_info, saliency_point"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFAUELSVqPPp"
      },
      "source": [
        "img_files = list(data_dir.glob(\"./*.jpg\"))\n",
        "images = [Image.open(x) for x in img_files]\n",
        "img = join_images(images, col_wrap=2, img_size=(128, -1))\n",
        "img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC3UKMuKRLO2"
      },
      "source": [
        "img.save(f\"{output_dir}/{filename}_h.jpeg\", \"JPEG\")\n",
        "model.plot_img_crops_using_img(img, topK=5, col_wrap=6)\n",
        "plt.savefig(f\"{output_dir}/{filename}_h_sm.jpeg\",bbox_inches=\"tight\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WCO-qsdXvcA"
      },
      "source": [
        "saliency_info,sp = saliency_point_to_info(f\"{output_dir}/{filename}_h.jpeg\", img_files, model, image_mode='horizontal')\n",
        "encoded_labels(saliency_info['race'],labels_encoder)\n",
        "decoded_labels(encoded_labels(saliency_info['race'],labels_encoder),labels_encoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUtYDeI3TwGp"
      },
      "source": [
        "images = [Image.open(x) for x in img_files]\n",
        "img = join_images(images, col_wrap=1, img_size=(128, -1))\n",
        "img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RSS45soUB_u"
      },
      "source": [
        "img.save(f\"{output_dir}/{filename}_v.jpeg\", \"JPEG\")\n",
        "model.plot_img_crops_using_img(img, topK=5, col_wrap=6)\n",
        "plt.savefig(f\"{output_dir}/{filename}_v_sm.jpeg\",bbox_inches=\"tight\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5h16udObm-f"
      },
      "source": [
        "saliency_point = model.get_output(Path(f\"{output_dir}/{filename}_v.jpeg\"))['salient_point']\n",
        "print(saliency_point)\n",
        "saliency_image = saliency_to_image(img, saliency_point, img_files, image_mode='vertical')\n",
        "saliency_filename = saliency_image.absolute()\n",
        "print(f'Image picked by saliency filter: {saliency_filename}')\n",
        "saliencyID = str(saliency_filename).split('/')[-1].replace('.jpg','')\n",
        "saliency_info = img_info(img_labels, int(saliencyID)-1)\n",
        "print(saliency_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRK9_ALGBo6q"
      },
      "source": [
        "# Evaluate horizontal and vertical padding invariance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9Fix9v8y771"
      },
      "source": [
        "## Load experiment history\n",
        "The experiment hisotry is stored in `{img_dir}/Twitter_saliency/FairFace_pairwise_tests.csv`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EyII9g0Mtpz"
      },
      "source": [
        "pairwise_tests_data = f'{img_dir}/Twitter_saliency/FairFace_pairwise_tests.csv'\n",
        "if os.path.exists(pairwise_tests_data):\n",
        "  pairwise_df = pd.read_csv(pairwise_tests_data)\n",
        "  print(f'Loaded pairwise experiments history from: {pairwise_tests_data} ...')\n",
        "  experiment_ids = list(pairwise_df['experiment_id'].values)\n",
        "  instance_ids   = list(pairwise_df['instance_id'].values)\n",
        "  img1           = list(pairwise_df['img1'].values)\n",
        "  img2           = list(pairwise_df['img2'].values)\n",
        "  baseline_h1    = list(pairwise_df['baseline_h1'].values)\n",
        "  baseline_h2    = list(pairwise_df['baseline_h2'].values)\n",
        "  baseline_v1    = list(pairwise_df['baseline_v1'].values)\n",
        "  baseline_v2    = list(pairwise_df['baseline_v2'].values)\n",
        "  saliency_out   = list(pairwise_df['saliency_out'].values)\n",
        "  combine_mode   = list(pairwise_df['combine_mode'].values)\n",
        "else:\n",
        "  pairwise_df = pd.DataFrame()\n",
        "  experiment_ids = []\n",
        "  instance_ids   = []\n",
        "  img1           = []\n",
        "  img2           = []\n",
        "  baseline_h1    = []\n",
        "  baseline_h2    = []\n",
        "  baseline_v1    = []\n",
        "  baseline_v2    = []\n",
        "  saliency_out   = []\n",
        "  combine_mode   = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "597AIo7yFRZ-"
      },
      "source": [
        "debug = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_eYMdL5MLp9"
      },
      "source": [
        "padding_eval = {'horizontal': {'padding_blocks': {1: {'max': 25, 'min': 0}}},\n",
        "                'vertical': {'padding_blocks': {1: {'max': 25, 'min': 0}}}} if debug else \\\n",
        "              {'horizontal': { \n",
        "                    'padding_blocks': {\n",
        "                         1: {'min': 0, 'max': 25},\n",
        "                         2: {'min': 25, 'max': 75},\n",
        "                         3: {'min': 75, 'max': 300},\n",
        "                      }\n",
        "                    },\n",
        "                 'vertical': { \n",
        "                     'padding_blocks': {\n",
        "                         1: {'min': 0, 'max': 25},\n",
        "                         2: {'min': 25, 'max': 75},\n",
        "                         3: {'min': 75, 'max': 300},\n",
        "                     }\n",
        "                   }\n",
        "               }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybi67tPDADZj"
      },
      "source": [
        "output_dir =f'{img_dir}/Twitter_saliency/FairFace_pairwise_tests/'\n",
        "num_eval = 1\n",
        "for i in range(len(padding_eval)):\n",
        "  eval_key = list(padding_eval.keys())[i]\n",
        "  label_id = eval_key\n",
        "  if  eval_key == 'horizontal':\n",
        "    label_id = 'h'\n",
        "    num_cols = 2\n",
        "  elif eval_key == 'vertical':\n",
        "    label_id = 'v'\n",
        "    num_cols = 1\n",
        "  padding_blocks = padding_eval[eval_key]['padding_blocks'] \n",
        "  for j in range(len(padding_blocks)):\n",
        "    for k in tqdm(range(num_eval)):\n",
        "      instance_id = randomID_generator()\n",
        "      image_files = glob.glob(str(data_dir / Path(\"./*.jpg\")))\n",
        "      random.SystemRandom().shuffle(image_files)\n",
        "      images = [Image.open(f)for f in image_files]\n",
        "      padding_ranges = padding_blocks[j+1]\n",
        "      padding = random.SystemRandom().choice(range(padding_ranges['min'],\n",
        "                                                   padding_ranges['max']))\n",
        "      print(f'Using a padding value: {padding}')\n",
        "      img = join_images(images, col_wrap=num_cols, img_size=(128,128),\n",
        "                        padding=padding)\n",
        "      filename = f'{instance_id}_{file_id}_p{padding}_t{k}_{label_id}'\n",
        "      output_file = f\"{output_dir}/{filename}.jpeg\"\n",
        "      img.save(output_file, \"JPEG\")\n",
        "      model.plot_img_crops_using_img(img, topK=5, col_wrap=6)\n",
        "      saliency_info,sp = saliency_point_to_info(output_file, img_files, model, image_mode='horizontal')\n",
        "      plt.savefig(f\"{output_dir}/{filename}_sm.jpeg\",bbox_inches=\"tight\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5pkn14YJ2uI"
      },
      "source": [
        "model.plot_img_crops(data_dir / Path(f\"{img_id1}.jpg\"), topK=2, aspectRatios=[0.56])\n",
        "plt.savefig(f\"{img_id1}_{img_race1}_{img_gender1}_saliency.jpeg\", bbox_inches=\"tight\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pArXZafuuKUt"
      },
      "source": [
        "model.plot_img_crops(data_dir / Path(f\"{img_id2}.jpg\"), topK=2, aspectRatios=[0.56])\n",
        "plt.savefig(f\"{img_id2}_{img_race2}_{img_gender2}_saliency.jpeg\", bbox_inches=\"tight\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONsvSt_idfRx"
      },
      "source": [
        "# Randomized saliency filter testing for padding invariance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWsk6_1xdj6V"
      },
      "source": [
        "## Null hypothesis\n",
        "**H₀** --> There are no differences between the baseline outputs of the saliency filter and the saliency filter outputs following randomized image paddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWCAW59iVQSv"
      },
      "source": [
        "## Methodology for generating randomized image pairs from FairFace data\n",
        "Randomization of the images for the pairwise comparisons are generated using the `random.SystemRandom()` class in the [Python **`random`** library](https://docs.python.org/3/library/random.html). \n",
        "\n",
        "The use of **`random.SystemRandom()`** class means, the exact image pairings are always dependent on the random numbers provided by the operating system sources. This method of random number generation is not available on all systems. Since this does not rely on the software state, the image pairing sequences are not reproducible. \n",
        "\n",
        "The goal of this experiment is to identify the existence of any statistical significant differences between the saliency filter outputs using baseline image pairs and the saliency filter outputs following randomized image padding. Therefore, the exact image pairing sequences used for the saliency filter output comparisons are immaterial for the reproducibility of this experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoeQN8tV7Ltl"
      },
      "source": [
        "num_pairwise_tests = 1 if debug else 2\n",
        "num_eval = 1 if debug else 10\n",
        "len(experiment_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLE7z2o5J2uN"
      },
      "source": [
        "for _ in tqdm(range(num_pairwise_tests)):\n",
        "  img_idx1,img_idx2 = random_imgID_generator(img_labels)\n",
        "  max_retries = 2000\n",
        "  img_idx1, img_idx2 = img_pairs_filter(img_labels,img_idx1,img_idx2,\n",
        "                                        max_retries=max_retries)\n",
        "  img1_info,img2_info = img_info(img_labels, img_idx1, img_idx2)\n",
        "\n",
        "  clear_image_history(unzip_dir)\n",
        "  get_fairface_img(img_labels, img_idx1, unzip_dir, fairface_data)\n",
        "  get_fairface_img(img_labels, img_idx2, unzip_dir, fairface_data)\n",
        "\n",
        "  img_id1 = str(img_labels.iloc[img_idx1].file).split('/')[-1].replace('.jpg','')\n",
        "  img_race1 = str(img_labels.iloc[img_idx1].race)\n",
        "  img_gender1 = str(img_labels.iloc[img_idx1].gender)\n",
        "\n",
        "  img_id2 = str(img_labels.iloc[img_idx2].file).split('/')[-1].replace('.jpg','')\n",
        "  img_race2 = str(img_labels.iloc[img_idx2].race)\n",
        "  img_gender2 = str(img_labels.iloc[img_idx2].gender)\n",
        "\n",
        "  file_id = f'{img_id1}_{img_race1}_{img_gender1}--{img_id2}_{img_race2}_{img_gender2}'\n",
        "  experiment_id = randomID_generator()\n",
        "\n",
        "  image_files = glob.glob(str(data_dir / Path(\"./*.jpg\")))\n",
        "\n",
        "  output_dir =f'{img_dir}/Twitter_saliency/FairFace_pairwise_tests/'\n",
        "  filename = f'{experiment_id}_{file_id}_{label_id}'\n",
        "\n",
        "  images = [Image.open(f)for f in image_files]\n",
        "  img = join_images(images, col_wrap=1, img_size=(128,128))\n",
        "  output_file = f\"{output_dir}/{filename}_baseline_v1.jpeg\"\n",
        "  img.save(output_file, \"JPEG\")\n",
        "  model.plot_img_crops_using_img(img, topK=5, col_wrap=6)\n",
        "  baselinev1_saliency_info,sp = saliency_point_to_info(Path(output_file).as_posix(), image_files, model, image_mode='vertical')\n",
        "  if debug:\n",
        "    print(image_files)\n",
        "    print(baselinev1_saliency_info,sp)\n",
        "  plt.savefig(f\"{output_dir}/{filename}_baseline_v1_sm.jpeg\",bbox_inches=\"tight\")\n",
        "  if not debug:\n",
        "    plt.close()\n",
        "  _=gc.collect()\n",
        "\n",
        "  image_files.reverse()\n",
        "  images = [Image.open(f)for f in image_files]\n",
        "  img = join_images(images, col_wrap=1, img_size=(128,128))\n",
        "  output_file = f\"{output_dir}/{filename}_baseline_v2.jpeg\"\n",
        "  img.save(output_file, \"JPEG\")\n",
        "  model.plot_img_crops_using_img(img, topK=5, col_wrap=6)\n",
        "  baselinev2_saliency_info,sp = saliency_point_to_info(Path(output_file).as_posix(), image_files, model, image_mode='vertical')\n",
        "  if debug:\n",
        "    print(image_files)\n",
        "    print(baselinev2_saliency_info,sp)\n",
        "  plt.savefig(f\"{output_dir}/{filename}_baseline_v2_sm.jpeg\",bbox_inches=\"tight\")\n",
        "  if not debug:\n",
        "    plt.close()\n",
        "  _=gc.collect()\n",
        "\n",
        "  images = [Image.open(f)for f in image_files]\n",
        "  img = join_images(images, col_wrap=2, img_size=(128,128))\n",
        "  output_file = f\"{output_dir}/{filename}_baseline_h1.jpeg\"\n",
        "  img.save(output_file, \"JPEG\")\n",
        "  model.plot_img_crops_using_img(img, topK=5, col_wrap=6)\n",
        "  baselineh1_saliency_info,sp = saliency_point_to_info(Path(output_file).as_posix(), image_files, model, image_mode='horizontal')\n",
        "  if debug:\n",
        "    print(image_files)\n",
        "    print(baselineh1_saliency_info,sp)\n",
        "  plt.savefig(f\"{output_dir}/{filename}_baseline_h1_sm.jpeg\",bbox_inches=\"tight\")\n",
        "  if not debug:\n",
        "    plt.close()\n",
        "  _=gc.collect()\n",
        "\n",
        "  image_files.reverse()\n",
        "  images = [Image.open(f)for f in image_files]\n",
        "  img = join_images(images, col_wrap=2, img_size=(128,128))\n",
        "  output_file = f\"{output_dir}/{filename}_baseline_h2.jpeg\"\n",
        "  img.save(output_file, \"JPEG\")\n",
        "  model.plot_img_crops_using_img(img, topK=5, col_wrap=6)\n",
        "  baselineh2_saliency_info,sp = saliency_point_to_info(Path(output_file).as_posix(), image_files, model, image_mode='horizontal')\n",
        "  if debug:\n",
        "    print(image_files)\n",
        "    print(baselineh2_saliency_info,sp)\n",
        "  plt.savefig(f\"{output_dir}/{filename}_baseline_h2_sm.jpeg\",bbox_inches=\"tight\")\n",
        "  if not debug:\n",
        "    plt.close()\n",
        "  _=gc.collect()\n",
        "\n",
        "  for i in range(len(padding_eval)):\n",
        "    eval_key = list(padding_eval.keys())[i]\n",
        "    label_id = eval_key\n",
        "    if  eval_key == 'horizontal':\n",
        "      label_id = 'h'\n",
        "      num_cols = 2\n",
        "    elif eval_key == 'vertical':\n",
        "      label_id = 'v'\n",
        "      num_cols = 1\n",
        "\n",
        "    padding_blocks = padding_eval[eval_key]['padding_blocks'] \n",
        "    for j in range(len(padding_blocks)):\n",
        "      for k in tqdm(range(num_eval)):\n",
        "        instance_id = randomID_generator()\n",
        "        random.SystemRandom().shuffle(image_files)\n",
        "        images = [Image.open(f)for f in image_files]\n",
        "        padding_ranges = padding_blocks[j+1]\n",
        "        padding = random.SystemRandom().choice(range(padding_ranges['min'],\n",
        "                                                     padding_ranges['max']))\n",
        "        img = join_images(images, col_wrap=num_cols, img_size=(128,128),\n",
        "                          padding=padding)\n",
        "        filename = f'{instance_id}_{file_id}_p{padding}_t{k}_{label_id}'\n",
        "        output_file = f\"{output_dir}/{filename}.jpeg\"\n",
        "        img.save(output_file, \"JPEG\")\n",
        "        model.plot_img_crops_using_img(img, topK=5, col_wrap=3)\n",
        "        sm_output_file = f\"{output_dir}/{filename}_sm.jpeg\"\n",
        "        plt.savefig(sm_output_file,bbox_inches=\"tight\")\n",
        "        saliency_info,sp = saliency_point_to_info(Path(output_file).as_posix(), image_files, model, image_mode=eval_key)\n",
        "        if debug:\n",
        "          print(image_files)\n",
        "          print(saliency_info,sp)\n",
        "        \n",
        "        experiment_ids.append(experiment_id)\n",
        "        instance_ids.append(instance_id)\n",
        "        img1.append(img1_info)\n",
        "        img2.append(img2_info)\n",
        "        baseline_h1.append(encoded_labels(baselineh1_saliency_info['race'],labels_encoder))\n",
        "        baseline_h2.append(encoded_labels(baselineh2_saliency_info['race'],labels_encoder))\n",
        "        baseline_v1.append(encoded_labels(baselinev1_saliency_info['race'],labels_encoder))\n",
        "        baseline_v2.append(encoded_labels(baselinev2_saliency_info['race'],labels_encoder))\n",
        "        saliency_out.append(encoded_labels(saliency_info['race'],labels_encoder))\n",
        "        combine_mode.append(eval_key)\n",
        "\n",
        "        if not debug:\n",
        "          plt.close()\n",
        "        _=gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KISS9HeA6mCC"
      },
      "source": [
        "pairwise_df = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ8bQqF9dz79"
      },
      "source": [
        "pairwise_df['experiment_id']= experiment_ids\n",
        "pairwise_df['instance_id']=instance_ids\n",
        "pairwise_df['img1']=img1\n",
        "pairwise_df['img2']=img2\n",
        "pairwise_df['baseline_h1']=baseline_h1\n",
        "pairwise_df['baseline_h2']=baseline_h2\n",
        "pairwise_df['baseline_v1']=baseline_v1\n",
        "pairwise_df['baseline_v2']=baseline_v2\n",
        "pairwise_df['saliency_out']=saliency_out\n",
        "pairwise_df['combine_mode']=combine_mode "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfTws95qdtsz"
      },
      "source": [
        "print(len(pairwise_df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqDguzpceJtY"
      },
      "source": [
        "# Calculate statistical significance\n",
        "\n",
        "[Wilcoxon signed rank test](https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test) is used to calculate whether there are any statistically significant differences between the baseline saliency filter outputs and the saliency filter outputs following image padding. The Wilcoxon signed rank test is performed using the [SciPy library](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wilcoxon.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtB-KSVMu8vx"
      },
      "source": [
        "w, p = wilcoxon(pairwise_df['baseline_h2']-pairwise_df['saliency_out'])\n",
        "print(w,p)\n",
        "pairwise_df['globalh2_wt_p'] = p\n",
        "pairwise_df['globalh2_wt_w'] = w\n",
        "w, p = wilcoxon(pairwise_df['baseline_h1']-pairwise_df['saliency_out'])\n",
        "print(w,p)\n",
        "pairwise_df['globalh1_wt_p'] = p\n",
        "pairwise_df['globalh1_wt_w'] = w\n",
        "w, p = wilcoxon(pairwise_df['baseline_v2']-pairwise_df['saliency_out'])\n",
        "print(w,p)\n",
        "pairwise_df['globalv2_wt_p'] = p\n",
        "pairwise_df['globalv2_wt_w'] = w\n",
        "w, p = wilcoxon(list(pairwise_df['baseline_v1']-pairwise_df['saliency_out']))\n",
        "print(w,p)\n",
        "pairwise_df['globalv1_wt_p'] = p\n",
        "pairwise_df['globalv1_wt_w'] = w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1y1hZhVvK4z"
      },
      "source": [
        "pairwise_df['localh2_wt_p'] = np.nan\n",
        "pairwise_df['localh2_wt_w'] = np.nan\n",
        "pairwise_df['localh1_wt_p'] = np.nan\n",
        "pairwise_df['localh1_wt_w'] = np.nan\n",
        "pairwise_df['localv2_wt_p'] = np.nan\n",
        "pairwise_df['localv2_wt_w'] = np.nan\n",
        "pairwise_df['localv1_wt_p'] = np.nan\n",
        "pairwise_df['localv1_wt_w'] = np.nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKSrTbgZrpsP"
      },
      "source": [
        "for expID in tqdm(list(set(list(pairwise_df.experiment_id.values)))):\n",
        "  condition = pairwise_df['experiment_id'] == expID\n",
        "\n",
        "  diff = list(pairwise_df.loc[condition,['baseline_h2']].values-pairwise_df.loc[condition,['saliency_out']].values)\n",
        "  diff = [list(d)[0]for d in diff]\n",
        "  try:\n",
        "    w, p = wilcoxon(diff)\n",
        "    pairwise_df.loc[condition,'localh2_wt_p'] = p\n",
        "    pairwise_df.loc[condition,'localh2_wt_w'] = w\n",
        "  except ValueError as e:\n",
        "    print(f'Skipping Wilcoxon Signed Rank test for: {expID} due to: \\n{e}')\n",
        "\n",
        "  diff = list(pairwise_df.loc[condition,['baseline_h1']].values-pairwise_df.loc[condition,['saliency_out']].values)\n",
        "  diff = [list(d)[0]for d in diff]\n",
        "  try:\n",
        "    w, p = wilcoxon(diff)\n",
        "    pairwise_df.loc[condition,'localh1_wt_p'] = p\n",
        "    pairwise_df.loc[condition,'localh1_wt_w'] = w\n",
        "  except ValueError as e:\n",
        "    print(f'Skipping Wilcoxon Signed Rank test for: {expID} due to: \\n{e}')\n",
        "\n",
        "  diff = list(pairwise_df.loc[condition,['baseline_v2']].values-pairwise_df.loc[condition,['saliency_out']].values)\n",
        "  diff = [list(d)[0]for d in diff]\n",
        "  try:\n",
        "    w, p = wilcoxon(diff)\n",
        "    pairwise_df.loc[condition,'localv2_wt_p'] = p\n",
        "    pairwise_df.loc[condition,'localv2_wt_w'] = w\n",
        "  except ValueError as e:\n",
        "    print(f'Skipping Wilcoxon Signed Rank test for: {expID} due to: \\n{e}')\n",
        "\n",
        "  diff = list(pairwise_df.loc[condition,['baseline_v1']].values-pairwise_df.loc[condition,['saliency_out']].values)\n",
        "  diff = [list(d)[0]for d in diff]\n",
        "  try:\n",
        "    w, p = wilcoxon(diff)\n",
        "    pairwise_df.loc[condition,'localv1_wt_p'] = p\n",
        "    pairwise_df.loc[condition,'localv1_wt_w'] = w\n",
        "  except ValueError as e:\n",
        "    print(f'Skipping Wilcoxon Signed Rank test for: {expID} due to: \\n{e}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmoRas4Hx7u1"
      },
      "source": [
        "# Save experiment history"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5JoUoqig4CN"
      },
      "source": [
        "pairwise_df.to_csv(pairwise_tests_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd6twiHsp8BH"
      },
      "source": [
        "print(len(pairwise_df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OddGi3ipbs56"
      },
      "source": [
        "pairwise_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uip4noGqyGkm"
      },
      "source": [
        "pairwise_df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX-kZ31Z5BOc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}